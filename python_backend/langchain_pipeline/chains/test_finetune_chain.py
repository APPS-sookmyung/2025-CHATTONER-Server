"""
FinetuneChain Test Script - Version with force_convert flag
Test LoRA fine-tuning model and ChatGPT 2-stage conversion pipeline

Execution method:
python test_finetune_chain.py
"""

import sys
import asyncio
import logging
from pathlib import Path
from datetime import datetime

# Project path configuration
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

from langchain_pipeline.chains.finetune_chain import FinetuneChain

# Logging configuration
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_should_use_lora():
    """Test LoRA usage conditions (modified logic)"""
    print("\nLoRA Usage Condition Unit Test")
    print("=" * 40)
    
    try:
        chain = FinetuneChain.__new__(FinetuneChain)
        
        test_cases = [
            # Formality level 5 or higher - always True
            ({"baseFormalityLevel": 5}, "personal", True),
            ({"baseFormalityLevel": 5}, "business", True),
            
            # Formality level 4 + business/report - True
            ({"baseFormalityLevel": 4}, "business", True),
            ({"baseFormalityLevel": 4}, "report", True),
            ({"baseFormalityLevel": 4}, "personal", False),  # personal is not allowed
            
            # Formality level 3 - False even for business/report
            ({"baseFormalityLevel": 3}, "business", False),
            ({"baseFormalityLevel": 3}, "report", False),
            ({"baseFormalityLevel": 3}, "personal", False),
            
            # formal_document_mode = True - always True
            ({"baseFormalityLevel": 2, "formal_document_mode": True}, "personal", True),
            ({"baseFormalityLevel": 1, "formal_document_mode": True}, "business", True),
            
            # sessionFormalityLevel takes priority
            ({"sessionFormalityLevel": 5, "baseFormalityLevel": 2}, "business", True),
            ({"sessionFormalityLevel": 3, "baseFormalityLevel": 5}, "business", False),
            
            # Empty profile (default 3) - False
            ({}, "business", False),
            ({}, "report", False),
            ({}, "personal", False),
        ]
        
        for profile, context, expected in test_cases:
            result = chain._should_use_lora(profile, context)
            status = "PASS" if result == expected else "FAIL"
            print(f"[{status}] í”„ë¡œí•„: {profile}, ì»¨í…ìŠ¤íŠ¸: {context} -> {result} (ì˜ˆìƒ: {expected})")
            
    except Exception as e:
        print(f"[ERROR] Unit test failed: {e}")

async def test_force_convert():
    """Test user explicit request"""
    print("\nUser Explicit Request Test")
    print("=" * 60)
    
    try:
        finetune_chain = FinetuneChain()
    except Exception as e:
        print(f"[ERROR] FinetuneChain initialization failed: {e}")
        return
    
    # Cases that originally don't meet conversion conditions
    force_test_cases = [
        {
            "input": "ì•ˆë…•! ì˜¤ëŠ˜ ë­í•´?",
            "profile": {"baseFormalityLevel": 1},  
            "context": "personal",  
            "description": "Casual + personal conversation (originally no conversion)"
        },
        {
            "input": "ã…‹ã…‹ã…‹ ì¬ë°Œë„¤ ã…ã… ğŸ˜‚",
            "profile": {"baseFormalityLevel": 2},
            "context": "casual",
            "description": "Casual message with emoticons"
        },
        {
            "input": "ë¹¨ë¦¬ë¹¨ë¦¬ í•´ì£¼ì„¸ìš”!!",
            "profile": {},  
            "context": "personal",
            "description": "Empty profile + urgent request"
        }
    ]
    
    for i, test_case in enumerate(force_test_cases, 1):
        print(f"\n[Force Conversion Test {i}] {test_case['description']}")
        print(f"ì›ë³¸: {test_case['input']}")
        
        # 1. Normal conversion (force_convert=False) - expected to fail
        try:
            result_normal = await finetune_chain.convert_to_formal(
                input_text=test_case['input'],
                user_profile=test_case['profile'],
                context=test_case['context'],
                force_convert=False
            )
            
            if result_normal['success']:
                print("Normal conversion: [SUCCESS] (unexpected)")
                print(f"   Result: {result_normal['converted_text']}")
            else:
                print(f"Normal conversion: [FAIL] {result_normal['error']}")
        except Exception as e:
            print(f"Normal conversion: [ERROR] {e}")
        
        # 2. Force conversion (force_convert=True) - expected to succeed
        try:
            result_forced = await finetune_chain.convert_to_formal(
                input_text=test_case['input'],
                user_profile=test_case['profile'],
                context=test_case['context'],
                force_convert=True
            )
            
            if result_forced['success']:
                print(f"Force conversion: [SUCCESS] ({result_forced['method']})")
                print(f"   Result: {result_forced['converted_text']}")
                print(f"   Conversion reason: {result_forced['reason']}")
                print(f"   Force mode: {result_forced['forced']}")
            else:
                print(f"Force conversion: [FAIL] {result_forced['error']}")
        except Exception as e:
            print(f"Force conversion: [ERROR] {e}")
        
        print("-" * 60)

async def test_convenience_method():
    """Test convenience methods"""
    print("\nConvenience Method Test")
    print("=" * 40)
    
    try:
        finetune_chain = FinetuneChain()
    except Exception as e:
        print(f"[ERROR] FinetuneChain initialization failed: {e}")
        return
    
    test_input = "ì•¼ ì´ê±° ì–¸ì œ í•˜ëƒ?"
    test_profile = {"baseFormalityLevel": 1}
    
    try:
        # Test convert_by_user_request method
        result = await finetune_chain.convert_by_user_request(
            input_text=test_input,
            user_profile=test_profile,
            context="personal"
        )
        
        if result['success']:
            print("[SUCCESS] Convenience method")
            print(f"Original: {test_input}")
            print(f"Converted: {result['converted_text']}")
            print(f"Force conversion: {result['forced']}")
            print(f"Conversion reason: {result['reason']}")
            print(f"Method used: {result['method']}")
        else:
            print(f"[FAIL] Convenience method: {result['error']}")
    except Exception as e:
        print(f"[ERROR] Convenience method: {e}")

async def test_finetune_chain():
    """FinetuneChain integration test"""
    print("=" * 60)
    print("FinetuneChain Integration Test Started")
    print("=" * 60)
    
    # 1. Initialize FinetuneChain
    try:
        print("\nInitializing FinetuneChain...")
        finetune_chain = FinetuneChain()
        print("[SUCCESS] FinetuneChain initialization completed")
    except Exception as e:
        print(f"[ERROR] FinetuneChain initialization failed: {e}")
        return
    
    # 2. Check system status
    print("\nSystem Status Check")
    print("=" * 40)
    status = finetune_chain.get_status()
    
    print(f"LoRA status: {status['lora_status']}")
    print(f"Services available: {status['services_available']}")
    print(f"Base model loaded: {status['base_model_loaded']}")
    print(f"Device: {status['device']}")
    print(f"LoRA model path: {status['lora_model_path']}")
    print(f"Model name: {status.get('model_name', 'N/A')}")
    
    # 3. Set test user profiles
    test_user_profiles = {
        "high_formal": {
            "baseFormalityLevel": 5, 
            "baseFriendlinessLevel": 3,
            "baseEmotionLevel": 2,    
            "baseDirectnessLevel": 4,
            "negativePreferences": {
                "avoidFloweryLanguage": "moderate",
                "avoidRepetitiveWords": "strict",
                "emoticonUsage": "strict"
            },
            "formal_document_mode": True
        },
        "medium_formal": {
            "baseFormalityLevel": 4,
            "baseFriendlinessLevel": 3,
            "baseEmotionLevel": 3,
            "baseDirectnessLevel": 3,
            "negativePreferences": {
                "avoidFloweryLanguage": "moderate"
            }
        },
        "low_formal": {
            "baseFormalityLevel": 2,  # ìºì£¼ì–¼ (LoRA ì‚¬ìš© ì•ˆë¨)
            "baseFriendlinessLevel": 4,
            "baseEmotionLevel": 4,
            "baseDirectnessLevel": 3
        }
    }
    
    # 4. ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜
    test_cases = [
        {
            "input": "ì•ˆë…•í•˜ì„¸ìš”! íšŒì˜ ì–¸ì œ í•˜ì£ ?",
            "context": "business",
            "description": "ìºì£¼ì–¼í•œ ì—…ë¬´ ë©”ì‹œì§€ -> ê³µì‹ ë¬¸ì„œ",
            "profile": "high_formal",
            "force_convert": False
        },
        {
            "input": "ë‚´ì¼ í”„ë¡œì íŠ¸ ëë‚´ì•¼ í•´ìš” ğŸ˜­ ë„ì™€ì£¼ì„¸ìš”",
            "context": "business", 
            "description": "ê°ì •ì  í‘œí˜„ -> ê³µì‹ì  ìš”ì²­ì„œ",
            "profile": "high_formal",
            "force_convert": False
        },
        {
            "input": "ì´ê±° ì™œ ì•ˆë˜ëŠ”ì§€ ëª¨ë¥´ê² ë„¤... í™•ì¸ ì¢€ í•´ì£¼ì„¸ìš”",
            "context": "report",
            "description": "ë¶ˆë§Œ í‘œí˜„ -> ê³µì‹ ë³´ê³ ì„œ",
            "profile": "medium_formal",
            "force_convert": False
        },
        {
            "input": "ê¸‰í•˜ê²Œ ì²˜ë¦¬í•´ì•¼ í•  ì¼ì´ ìˆì–´ì„œ ì—°ë½ë“œë ¤ìš”",
            "context": "business",
            "description": "ê¸‰ë°•í•œ ìƒí™© -> ì •ì¤‘í•œ ìš”ì²­",
            "profile": "high_formal",
            "force_convert": False
        },
        {
            "input": "ì¢€ ëŠ¦ì„ ê²ƒ ê°™ì•„ìš” ë¯¸ì•ˆí•´ìš”",
            "context": "personal",  # ê°œì¸ì  -> LoRA ì‚¬ìš© ì•ˆë¨
            "description": "ê°œì¸ì  ì‚¬ê³¼ -> ìºì£¼ì–¼ ìœ ì§€",
            "profile": "low_formal",
            "force_convert": False
        },
        # ê°•ì œ ë³€í™˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¶”ê°€
        {
            "input": "ã…‹ã…‹ ê·¸ê±° ì–¸ì œ ëë‚˜ìš”? ğŸ˜…",
            "context": "personal",
            "description": "ì´ëª¨í‹°ì½˜ í¬í•¨ -> ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ê³µì‹í™”",
            "profile": "low_formal",
            "force_convert": True
        },
        {
            "input": "ì•„ ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ğŸ˜¤",
            "context": "personal", 
            "description": "ê°ì • í‘œí˜„ -> ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ê³µì‹í™”",
            "profile": "low_formal",
            "force_convert": True
        }
    ]
    
    # 5. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print(f"\ní…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰ ({len(test_cases)}ê°œ)")
    print("=" * 60)
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n[í…ŒìŠ¤íŠ¸ {i}] {test_case['description']}")
        print(f"ì›ë³¸: {test_case['input']}")
        print(f"ì»¨í…ìŠ¤íŠ¸: {test_case['context']}")
        print(f"í”„ë¡œí•„: {test_case['profile']}")
        print(f"ê°•ì œ ë³€í™˜: {test_case['force_convert']}")
        
        user_profile = test_user_profiles[test_case['profile']]
        
        try:
            # ê³µì‹ ë¬¸ì„œ ë³€í™˜ ì‹¤í–‰
            start_time = datetime.now()
            result = await finetune_chain.convert_to_formal(
                input_text=test_case['input'],
                user_profile=user_profile,
                context=test_case['context'],
                force_convert=test_case['force_convert']
            )
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            # ê²°ê³¼ ì¶œë ¥
            if result['success']:
                print(f"[ì„±ê³µ] ë³€í™˜ ì™„ë£Œ ({duration:.2f}ì´ˆ)")
                print(f"ë³€í™˜ ë°©ë²•: {result['method']}")
                print(f"ë³€í™˜ ì´ìœ : {result['reason']}")
                print(f"ìµœì¢… ê²°ê³¼: {result['converted_text']}")
                
                # LoRA 1ì°¨ ë³€í™˜ ê²°ê³¼ë„ í‘œì‹œ
                if result.get('lora_output') and result['lora_output'] != result['converted_text']:
                    print(f"LoRA 1ì°¨ ë³€í™˜: {result['lora_output']}")
                
            else:
                print(f"[ì‹¤íŒ¨] ë³€í™˜ ì‹¤íŒ¨: {result['error']}")
                print(f"ì‚¬ìš©ëœ ë°©ë²•: {result['method']}")
                print(f"ì‹¤íŒ¨ ì´ìœ : {result['reason']}")
                
        except Exception as e:
            print(f"[ì˜¤ë¥˜] í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        
        print("-" * 60)
    
    # 6. ì¡°ê±´ë³„ ë³€í™˜ í…ŒìŠ¤íŠ¸
    print("\nì¡°ê±´ë³„ ë³€í™˜ í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    condition_tests = [
        {
            "name": "ê²©ì‹ë„ 5 + business (ë¬´ì¡°ê±´ True)",
            "profile": {"baseFormalityLevel": 5},
            "context": "business",
            "should_use_lora": True
        },
        {
            "name": "ê²©ì‹ë„ 4 + business (True)", 
            "profile": {"baseFormalityLevel": 4},
            "context": "business",
            "should_use_lora": True
        },
        {
            "name": "ê²©ì‹ë„ 4 + personal (False)",
            "profile": {"baseFormalityLevel": 4},
            "context": "personal", 
            "should_use_lora": False
        },
        {
            "name": "ê²©ì‹ë„ 3 + business (False)",
            "profile": {"baseFormalityLevel": 3},
            "context": "business",
            "should_use_lora": False
        },
        {
            "name": "ë¹ˆ í”„ë¡œí•„ + business (False)",
            "profile": {},
            "context": "business",
            "should_use_lora": False
        },
        {
            "name": "formal_document_mode = True",
            "profile": {"baseFormalityLevel": 2, "formal_document_mode": True},
            "context": "personal",
            "should_use_lora": True
        }
    ]
    
    for test in condition_tests:
        should_use = finetune_chain._should_use_lora(test['profile'], test['context'])
        label = "PASS" if should_use == test['should_use_lora'] else "FAIL"
        print(f"[{label}] {test['name']}: LoRA usage {should_use} (expected: {test['should_use_lora']})")
    
    # 7. Performance and memory information
    print("\nPerformance Information")
    print("=" * 40)
    
    device = status.get('device', 'unknown')
    if device == 'cuda':
        # Query only local GPU information
        try:
            import torch
            print(f"GPU memory usage: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
            print(f"GPU memory max: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")
            print(f"GPU device: {torch.cuda.get_device_name()}")
        except Exception as e:
            print(f"[WARNING] Failed to query GPU information: {e}")
    else:
        print("Running in CPU mode")
    
    print("\nFinetuneChain basic test completed!")

async def main():
    """Main test execution"""
    print("FinetuneChain Full Test Started!")
    print("=" * 80)
    
    # 1. Condition tests
    test_should_use_lora()
    
    # 2. Main integration test
    await test_finetune_chain()
    
    # 3. Force conversion test
    await test_force_convert()
    
    # 4. Convenience method test
    await test_convenience_method()
    
    print("\n" + "=" * 80)
    print("All tests completed!")
    print("=" * 80)

if __name__ == "__main__":
    asyncio.run(main())